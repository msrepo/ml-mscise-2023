{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4iN6EjGl9oixWVLgjx0dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msrepo/ml-mscise-2023/blob/master/Lecture8_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 8: Naive Bayes\n",
        "\n",
        "Adapted from/ Original Notebook by:\n",
        "**Volodymyr Kuleshov**\n",
        "*Applied Machine Learning*\n",
        "\n"
      ],
      "metadata": {
        "id": "uV3hgNu30jOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Text Classification**\n",
        "\n",
        "We will now do a quick detour to talk about an important application area of machine learning text classification. \n",
        "\n",
        "Afterwards, we will see how text classification motivates new classification algorithms.\n",
        "\n",
        "An interesting instance of a classification problem is classifying text.\n",
        "-  includes a lot of applied problems: spam filtering, fraud detection, medical record classification etc.\n",
        "- input $x$ are sequences of words of arbitrary length\n",
        "- The dimensionality of text inputs is usually very large, proportional to the size of the vocabulary\n",
        "\n",
        "**Classification Dataset: Twenty Newsgroups**\n",
        "\n",
        "To illustrate the text classification problem, we will use a popular dataset called *20-newsgroups*\n",
        "\n",
        "- It contains ~20,000 documents collected approximately evenly from 20 different online newsgroups\n",
        "- Each newgroup covers a different topic such as medicine, computer graphics, or religion\n",
        "- This dataset is widely used to benchmark text classification and other types of algorithms\n",
        "\n",
        "Lets load this dataset.\n"
      ],
      "metadata": {
        "id": "jxIU9SgP0zKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOE86lLp0e7F",
        "outputId": "a45b47fb-3db5-424a-e8d0-312fd2ad064e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# for this lecture, we will restrict our attention to just 4 different newsgroups\n",
        "categories = ['misc.forsale', 'rec.motorcycles', 'comp.graphics', 'sci.med']\n",
        "\n",
        "# load the dataset\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0)\n",
        "\n",
        "print(twenty_train.DESCR[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(twenty_train.target_names)\n",
        "print(twenty_train.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUy35Cwa2oWm",
        "outputId": "c5069f0a-8d95-4ae9-bbef-65f63552f9a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.graphics', 'misc.forsale', 'rec.motorcycles', 'sci.med']\n",
            "Subject: Ovarian cancer treatment centers\n",
            "From: <RBPRMA@rohvm1.rohmhaas.com>\n",
            "Organization: Rohm and Haas Company\n",
            "Lines: 9\n",
            "\n",
            "A relative of mine has recently been diagnosed with \"stage 3 papillary cell\n",
            "ovarian cancer\".  We are urgently seeking the best place in the country for\n",
            "treatment for this.\n",
            "\n",
            "Does anyone have any suggestions?\n",
            "\n",
            "As you might suspect, time is of the essence.\n",
            "\n",
            "Thanks for your help.                                      Bob\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Representations for Text**\n",
        "\n",
        "Each data point $x$ in this dataset is a sequence of characters of an arbitrary length.\n",
        "\n",
        "How do we transform these into a $d$-dimensional features $\\phi(x)$ that can be used with our machine learning algorithms?\n",
        "\n",
        "- we may devise hand-crafted features by inspecting the data:\n",
        "  - Does the message contain the word \"bike\"? Does the email of the user belong to a university?\n",
        "- We can count the number of occurrences of each word:\n",
        "  - Does this message contain \"Apple\", yes or no?\n",
        "- Finally, many modern deep learning methods can directly work with sequences of characters of an arbitrary length.\n",
        "\n",
        "\n",
        "**Bag of Words Representations**\n",
        "\n",
        "Perhaps the most widely used approach to representing text documents is called \"bag of words\".\n",
        "\n",
        "We start by defining a vocabulary $V$ containing all the possible words we are interested in e.g. $$V = \\{ \\text{cancer}, \\text{slow}, \\ldots\\}$$\n",
        "\n",
        "A bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector $\\phi(x) = \\left(\\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\\\end{array}\\right)$ of dimension $|V|$. The $j^{th}$ component $\\phi(x)_j$ equals $1$ if $x$ contains the $j$th word in $V$ and $0$ otherwise.\n",
        "\n",
        "Lets see an example of this approach on \"20-newsgroups\".\n",
        "\n",
        "We start by computing these features using the *sklearn* library."
      ],
      "metadata": {
        "id": "bRQlSZx92zEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorize the training set\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "X_train = count_vect.fit_transform(twenty_train.data)\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ul-GmCO8pZZ",
        "outputId": "2e68b626-642f-4257-839d-8e47d826cf8b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2361, 35807)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In *sklearn*, we can retrieve the index of $\\phi(x)$ associated with each *word* using the expression `count_vect.vocabulary_.get(word)`"
      ],
      "metadata": {
        "id": "ZY8ecNSQ9OzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Index for the word \"cancer\"',count_vect.vocabulary_.get(u'cancer'))\n",
        "print('Index for the word \"computer\"',count_vect.vocabulary_.get(u'computer'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqR25Mad9gjq",
        "outputId": "a9b3be2b-6638-4c87-fc86-43c6c5cc7acc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index for the word \"cancer\" 8612\n",
            "Index for the word \"computer\" 10160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our featurized dataset is in the matrix `X_train`. We can use the above indices to retrieve the 0-1 value that has been computed for each word."
      ],
      "metadata": {
        "id": "8V4ghkT093Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can examine if any of these words are present in our previous data point\n",
        "print(twenty_train.data[0])\n",
        "\n",
        "# lets see if it contans these two words?\n",
        "print('----'*20)\n",
        "print('Value at the index for the word \"treatment\"',X_train[0,count_vect.vocabulary_.get(u'treatment')])\n",
        "print('Value at the index for the word \"cancer\"',X_train[0, count_vect.vocabulary_.get(u'cancer')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8_PEkfM90Zh",
        "outputId": "d7d45fda-c833-42db-9202-5ad89ef69e0b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Ovarian cancer treatment centers\n",
            "From: <RBPRMA@rohvm1.rohmhaas.com>\n",
            "Organization: Rohm and Haas Company\n",
            "Lines: 9\n",
            "\n",
            "A relative of mine has recently been diagnosed with \"stage 3 papillary cell\n",
            "ovarian cancer\".  We are urgently seeking the best place in the country for\n",
            "treatment for this.\n",
            "\n",
            "Does anyone have any suggestions?\n",
            "\n",
            "As you might suspect, time is of the essence.\n",
            "\n",
            "Thanks for your help.                                      Bob\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Value at the index for the word \"treatment\" 1\n",
            "Value at the index for the word \"cancer\" 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Considerations**\n",
        "\n",
        "In practice, we may use some additional modifications of this technique.\n",
        "\n",
        "- Sometimes, the feature $\\phi(x)_j$ for the j-th word holds the count of word $j$ instead of just the binary occurence\n",
        "- The raw text is usually preprocessed. One common technique is *stemming*, in which we only keep the rrot of the word.\n",
        " - e.g. 'slowly' 'slowness', both map to 'slow\n",
        "- Filtering for common *stopwords* such as 'the', 'a', 'and'. Similarly, rare words are also typically excluded.\n",
        "\n",
        "\n",
        "\n",
        "**Classification using BoW features**\n",
        "\n",
        "Lets now have a look at the performance of classification over bag of words features.\n",
        "\n",
        "Now that we have a feature representation $\\phi(x)$, we can apply the classifier of our choice, such as logistic regression."
      ],
      "metadata": {
        "id": "3P_y-R3pM8TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create an instance of Softmax and fit the data\n",
        "logreg = LogisticRegression(C=1e5,multi_class='multinomial',verbose=True)\n",
        "logreg.fit(X_train, twenty_train.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIm8EH36M9gu",
        "outputId": "7d993cab-69ff-4bd2-edac-42334f8e8ab4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, multi_class='multinomial', verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_new = ['Long COVID has a lot of long-term effects.', \n",
        "            'New baby shoes. never worn.',\n",
        "           ]\n",
        "\n",
        "X_new = count_vect.transform(docs_new)\n",
        "predicted = logreg.predict(X_new)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print(f'{doc} => {twenty_train.target_names[category]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjAyMx4-PrPQ",
        "outputId": "097f50cb-cdb2-4818-a45b-8276a9783c80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long COVID has a lot of long-term effects. => sci.med\n",
            "New baby shoes. never worn. => misc.forsale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Text Classification**\n",
        "\n",
        "- Classifying text normally requires specifying features over the raw data.\n",
        "\n",
        "- A widely used representation is 'bag of words', in which features are occurences or counts of words.\n",
        "\n",
        "-  Once text if featured, any off-the-shelf supervised learning algorithm can be applied, but some work better than others, as we will see next.\n",
        "\n",
        "## Part 2: Naive Bayes\n",
        "Next, we are going to look at Naive Bayes - a generative classification algorithm. We will apply Naive Bayes to the text classification problem.\n",
        "\n",
        "**Review: Generative Models**\n",
        "\n",
        "There are two types of probabilistic models: *generative* and *discriminative*.\n",
        "\n",
        "$$\n",
        "\\text{Generative Model};  P_\\theta(x,y):\\mathcal{X} \\times \\mathcal{Y} \\to [0,1] \\\\\n",
        "\\text{discriminative Model}; P_\\theta(y|x):\\mathcal{X} \\times \\mathcal{Y} \\to [0,1]\n",
        "$$\n",
        "\n",
        "Given a new datapoint $x'$, we can match it against each class model and find the class that looks most similar to it:\n",
        "\n",
        "$$\n",
        " \\arg\\max_y \\log p(y|x) = \\arg \\max_y \\log \\frac{p(x|y)p(y)}{p(x)} = \\arg \\max_y \\log p(x|y) p(y)\n",
        "$$\n",
        "\n",
        "where we have applied Bayes rule in the second equation.\n"
      ],
      "metadata": {
        "id": "6XM-LSJaQlgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review: Gaussian Discriminant Model**\n",
        "\n",
        "The GDA algorithm defines the following model family:\n",
        "\n",
        "- the probability $P(x|y=k)$ of the data under class $k$ is a multivariate Gaussian $\\mathcal{N}(x;\\mu_k,\\textstyle \\sum_k)$ with parameters $\\mu_k$, $\\sum_k$.\n",
        "\n",
        "- the distribution over classes is Categorical, denoted $\\text{Categorical}(\\phi_1,\\phi_2,\\ldots,\\phi_n)$. Thus, $P_y(y=k) = \\phi_k$. Thus, $P_{\\theta}(x,y)$ is a mixture of $K$ Gaussians.\n",
        "\n",
        "$$\n",
        " P_{\\theta}(x,y) = \\sum_{k=1}^{K}P_{\\theta}(y=k)P_{\\theta}(x|y=k) = \\sum_{k=1}^{K}\\phi_k\\mathcal{N}(x;\\mu_k,\\sum_k)\n",
        "$$\n",
        "\n",
        "**Naive Bayes Assumption**\n",
        "In order to deal with high-dimensional $x$, we simplify the problem by making the *Naive Bayes* assumption.\n",
        "\n",
        "$$\n",
        "p(x|y) = \\prod_{j=1}^dp(x_j|y)\n",
        "$$\n",
        "\n",
        "In other words, the probability $p(x|y)$ factorizes over each dimension.\n",
        "\n",
        "- For example, if $x$ is a binary bag of words representation, then $p(x_j|y)$ is the probability of seeing the j-the word.\n",
        "\n",
        "- We can model each $p(x_j|y)$ via a bernoulli distribution, which has only one parameter.\n",
        "\n",
        "- Hence, it takes only $d$ parameters (instead of $2^d-1$) to specify the entire distribution $p(x|y) = \\prod_{j=1}^dp(x_j|y)$\n",
        "\n",
        "**Bernoulli Naive Bayes Model**\n",
        "\n",
        "We can apply the Naive Bayes assumption to obtain a model for when $x$ is in a bag of words representation.\n",
        "\n",
        "**Review: Maximum Likelihood Learning**\n",
        "\n",
        "In order to fit probabilistic models, we use the following objective:\n",
        "\n",
        "$$\n",
        " \\max_\\theta \\mathbb{E}_{x,y \\sim \\mathbb{P}_{data}}\\log P_\\theta(x,y)\n",
        "$$\n",
        "\n",
        "This seeks to find a model that assigns high probability to the training data.\n",
        "\n",
        "Let's use maximum likelihood to fit the Bernoulli Naive Bayes model. Note that model parameters $\\theta$ are the union of the parameters of each sub-model.\n",
        "\n",
        "$$\n",
        "\\theta = ( \\phi_1,\\phi_2,\\ldots,\\phi_K, \\psi_{11},\\psi_{21},\\ldots, \\psi_{dK} )\n",
        "$$\n",
        "\n",
        "**Learning a Bernoulli Naive Bayes Model**\n",
        "\n",
        "Given a dataset $\\mathcal{D} = \\{(x^{(i)},y^{(i)})|i=1,2,\\ldots,n \\}$, we want to optimize the log-likelihood $l(\\theta) = \\log L(\\theta)$"
      ],
      "metadata": {
        "id": "mDAHL0oGZWwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# for this lecture, we will restrict our attention to just 4 different newsgroups\n",
        "categories = ['misc.forsale', 'rec.motorcycles', 'comp.graphics', 'sci.med']\n",
        "\n",
        "# load the dataset\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "DrXy42n-ZWBU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vectorize the training set\n",
        "count_vect = CountVectorizer(binary=True, max_features=1000)\n",
        "y_train = twenty_train.target\n",
        "X_train = count_vect.fit_transform(twenty_train.data).toarray()\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C56G5q1mR5G-",
        "outputId": "b4729de5-7b8d-4b98-de80-a188481b79a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2361, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets compute the maximum likelihood model parameters on our dataset."
      ],
      "metadata": {
        "id": "oKokO_9ghja9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = X_train.shape[0]\n",
        "d = X_train.shape[1]\n",
        "k = len(categories)\n",
        "print(f'n {n} d {d} k {k}')\n",
        "\n",
        "# these are the shapes of the parameters\n",
        "psis = np.zeros([k,d])\n",
        "phis = np.zeros([k])\n",
        "\n",
        "# we can now compute the parameters\n",
        "for idx in range(k):\n",
        "  X_k = X_train[y_train==idx]\n",
        "  psis[idx] = np.mean(X_k,axis=0)\n",
        "  phis[idx] = X_k.shape[0] / float(n)\n",
        "\n",
        "# print out class proportions\n",
        "print(r'ϕ',phis)\n",
        "print(r'ψ',psis)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3JUn90ih4OP",
        "outputId": "fe98f1ab-ed40-4bd9-9119-7dbc63646f3d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n 2361 d 1000 k 4\n",
            "ϕ [0.24735282 0.24777637 0.25328251 0.25158831]\n",
            "ψ [[0.03424658 0.01883562 0.0890411  ... 0.4640411  0.16267123 0.0119863 ]\n",
            " [0.14871795 0.04957265 0.19316239 ... 0.46666667 0.19487179 0.00854701]\n",
            " [0.03344482 0.02341137 0.09698997 ... 0.63545151 0.31772575 0.04013378]\n",
            " [0.03198653 0.03198653 0.11111111 ... 0.54882155 0.29292929 0.02020202]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compute predictions using Bayes rule."
      ],
      "metadata": {
        "id": "742od5EjiC9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(x, psis, phis):\n",
        "  \"\"\"This returns class assignments and scores under the NB model.\n",
        "\n",
        "  We compute \\arg \\max_y p(y|x) as \\arg \\max_y p(x|y)p(y)\n",
        "  \"\"\"\n",
        "  # adjust shapes\n",
        "  n, d = x.shape\n",
        "  x =np.reshape(x,(1,n,d))\n",
        "  psis = np.reshape(psis, (k,1,d))\n",
        "\n",
        "  # clip probabilities to avoid log(0)\n",
        "  psis = psis.clip(1e-14, 1-1e-14)\n",
        "\n",
        "  # compute log probabilities\n",
        "  logpy = np.log(phis).reshape([k,1])\n",
        "  logpxy = x * np.log(psis) + (1-x)*np.log(1-psis)\n",
        "  logpyx = logpxy.sum(axis=2) + logpy\n",
        "\n",
        "  return logpyx.argmax(axis=0).flatten(), logpyx.reshape([k,n])\n",
        "\n",
        "idx, logpyx = naive_bayes_predict(X_train, psis, phis)\n",
        "print('Predictions',idx[:10])\n",
        "acc = (idx == y_train).mean()\n",
        "print(f'Accuracy: {acc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfkjPKUhuTfZ",
        "outputId": "b08935c1-9d10-4d2d-c254-5e0147553da3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions [0 1 3 1 0 2 2 0 1 0]\n",
            "Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQDISKxfvLee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}